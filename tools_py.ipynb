{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tools.py",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNk7xx5JPn5gq4kEFISeel/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sai-vaishnavi30/couresera/blob/master/tools_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6gIP6Hveze3"
      },
      "source": [
        "from skimage import io, transform\n",
        "import glob,math\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import time\n",
        "from sklearn import preprocessing\n",
        "import random\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.optimizers import SGD,Adam\n",
        "import numpy as np\n",
        "from keras import backend as K\n",
        "\n",
        "import gc\n",
        "from PIL import Image,ImageEnhance\n",
        "\n",
        "def read_image(imagePath, width=600, height=600, normalization = True):\n",
        "    img = io.imread(imagePath.split('\\n')[0])\n",
        "    if normalization == True:\n",
        "        imageData = transform.resize(img,(width, height, 3))\n",
        "        imageData = np.transpose(imageData,(2,0,1))\n",
        "        imageData[0] = preprocessing.scale(imageData[0])\n",
        "        imageData[1] = preprocessing.scale(imageData[1])\n",
        "        imageData[2] = preprocessing.scale(imageData[2])\n",
        "        imageData = np.transpose(imageData,(1,2,0))\n",
        "        # imageData = transform.resize(img,(width, height,3))\n",
        "    else:\n",
        "        imageData = transform.resize(img,(width, height,3))\n",
        "    return imageData\n",
        "\n",
        "def dataAugmentation(input_array, flip_left_right_rate=0.5, flip_top_bottom_rate=0.5):\n",
        "    # array转Image\n",
        "    if (random.random() < flip_left_right_rate):\n",
        "        # 左右翻转\n",
        "        input_array = np.transpose(input_array,(2,0,1))# 源w,h,3- > 3,w,h\n",
        "        input_array[0] = np.flip(input_array[0], 1)\n",
        "        input_array[1] = np.flip(input_array[1], 1)\n",
        "        input_array[2] = np.flip(input_array[2], 1)\n",
        "        input_array = np.transpose(input_array, (1,2,0))#源3, w, h -> w, h, 3\n",
        "    if (random.random() < flip_top_bottom_rate):\n",
        "        # 上下翻转\n",
        "        input_array = np.transpose(input_array, (2, 0, 1))  # 源w,h,3- > 3,w,h\n",
        "        input_array[0] = np.flip(input_array[0], 0)\n",
        "        input_array[1] = np.flip(input_array[1], 0)\n",
        "        input_array[2] = np.flip(input_array[2], 0)\n",
        "        input_array = np.transpose(input_array, (1, 2, 0))  # 源3, w, h -> w, h, 3\n",
        "\n",
        "    return np.array(input_array)\n",
        "\n",
        "def read_bottleneck(bottleneckPath):\n",
        "    bottleneck = []\n",
        "    bottleneckFile  =  open(bottleneckPath.split('\\n')[0])\n",
        "    content = bottleneckFile.read()\n",
        "    feature = content.split(',')\n",
        "    for i in feature[0:-1]:\n",
        "        bottleneck.append(float(i))\n",
        "    bottleneck = preprocessing.scale(bottleneck)\n",
        "    bottleneck = np.asarray(bottleneck)\n",
        "    return bottleneck\n",
        "\n",
        "#添加data以及label\n",
        "def add_array(arr1,arr2):\n",
        "    for a in arr2:\n",
        "        arr1.append(a)\n",
        "    return arr1\n",
        "\n",
        "\n",
        "#根据给定的key读取相应的文件名以及打上标签\n",
        "def create_data_label(dic,key,label_id):\n",
        "    data_path = []\n",
        "    label = []\n",
        "    for i in key:\n",
        "        for v in dic[i]:\n",
        "            label.append(label_id)\n",
        "            data_path.append(v)\n",
        "    return data_path, label\n",
        "\n",
        "#输出是某一类 train 和 test的文件路径的key\n",
        "def slice_train_test(array, i, K):\n",
        "    length = len(array)\n",
        "    step = math.floor(length / K)\n",
        "    if step == 0:\n",
        "        step = 1\n",
        "    print('总长度%d为步长%d' %(length, step))\n",
        "    train = []\n",
        "    test = []\n",
        "    if i==(K-1):\n",
        "        for x in array[0: (min((step * i), (length - 1)))]:\n",
        "            train.append(x)\n",
        "        for x in array[(min((step * i), (length - 1))):]:\n",
        "            test.append(x)\n",
        "    else:\n",
        "        for x in array[0: (min((step * i),(length - 1)))]:\n",
        "            train.append(x)\n",
        "\n",
        "        for x in array[(min((step * i),(length - 1))):(min((step * (i + 1),(length - 1))))]:\n",
        "            test.append(x)\n",
        "\n",
        "        for x in array[(min((step * (i + 1),(length - 1)))): ]:\n",
        "            train.append(x)\n",
        "    return train, test\n",
        "\n",
        "\n",
        "'''\n",
        "generator的输入是所有train/test的 path,label\n",
        "输出 batch 大小的 np.array data,label\n",
        "'''\n",
        "def batch_generator(all_data, all_label, batch_size, shuffle, class_num = 6,train = True):\n",
        "    assert len(all_data) == len(all_label)\n",
        "    print(len(all_data))\n",
        "    if shuffle:\n",
        "        indices = np.arange(len(all_data))\n",
        "        random.shuffle(indices)\n",
        "    while True:\n",
        "        for start_idx in range(0, len(all_data) - batch_size + 1, batch_size):\n",
        "            data = []\n",
        "            labels = []\n",
        "            if shuffle:\n",
        "                excerpt = indices[start_idx:start_idx + batch_size]\n",
        "            else:\n",
        "                excerpt = slice(start_idx, start_idx + batch_size)\n",
        "\n",
        "            for di in excerpt:\n",
        "                tmp_data = all_data[di]\n",
        "                if train:\n",
        "                    tmp_data = dataAugmentation(tmp_data)\n",
        "                data.append(tmp_data)\n",
        "\n",
        "            for li in excerpt:\n",
        "                cla = all_label[li]\n",
        "                tmp = [0 for x in range(class_num)]\n",
        "                tmp[cla] = 1\n",
        "                labels.append(tmp)\n",
        "\n",
        "            yield np.array(data),np.array(labels)\n",
        "\n",
        "            \n",
        "def as_keras_metric(method):\n",
        "    import functools\n",
        "    from keras import backend as K\n",
        "    import tensorflow as tf\n",
        "    @functools.wraps(method)\n",
        "    def wrapper(self, args, **kwargs):\n",
        "        \"\"\" Wrapper for turning tensorflow metrics into keras metrics \"\"\"\n",
        "        value, update_op = method(self, args, **kwargs)\n",
        "        K.get_session().run(tf.local_variables_initializer())\n",
        "        with tf.control_dependencies([update_op]):\n",
        "            value = tf.identity(value)\n",
        "        return value\n",
        "    return wrapper\n",
        "\n",
        "\n",
        "def batch_generator_confusion_matrix(all_data, all_label, batch_size, shuffle, class_num = 6):\n",
        "    assert len(all_data) == len(all_label)\n",
        "    print(len(all_data))\n",
        "\n",
        "    if shuffle:\n",
        "        indices = np.arange(len(all_data))\n",
        "        random.shuffle(indices)\n",
        "\n",
        "    for start_idx in range(0, len(all_data) - batch_size + 1, batch_size):\n",
        "        data = []\n",
        "        labels = []\n",
        "        if shuffle:\n",
        "            excerpt = indices[start_idx:start_idx + batch_size]\n",
        "        else:\n",
        "            excerpt = slice(start_idx, start_idx + batch_size)\n",
        "\n",
        "        for di in excerpt:\n",
        "            tmp_data = all_data[di]\n",
        "\n",
        "            data.append(all_data[di])\n",
        "\n",
        "        for li in excerpt:\n",
        "            cla = all_label[li]\n",
        "            tmp = [0 for x in range(class_num)]\n",
        "            tmp[cla] = 1\n",
        "            labels.append(tmp)\n",
        "\n",
        "        yield np.array(data), np.array(labels)\n",
        "\n",
        "def create_directory(dir_path):\n",
        "  if not os.path.exists(dir_path):\n",
        "    os.makedirs(dir_path)\n",
        "\n",
        "#返回np.array中最大数字的下标\n",
        "#如果arr是list 记得要转为array\n",
        "def arr_max_index(arr):\n",
        "    return np.where(arr == max(arr))[0][0]"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKyr1kg5e104"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGnfrSCHe-0E"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vamUuQWmfGWj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGgAgBQOfLP_"
      },
      "source": [
        ""
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6XNWearflgA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nf0XZJlig5Vz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KyI8XKtShNPi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}